{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBgGzvz8sFneaDNMQmM+PU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeoDinga/DL/blob/main/pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTAjevanteLj"
      },
      "outputs": [],
      "source": [
        "# Pytorch Tutorial\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORKING WITH DATA\n"
      ],
      "metadata": {
        "id": "D7jZfyTVuJXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download TRAINING data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download TEST data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "E8uqiZa6tweG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1B9ilpXuDL1",
        "outputId": "41268cdf-9999-4ec6-a7ff-c473cb7f2ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING MODELS"
      ],
      "metadata": {
        "id": "DMXdR0gouNP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define a neural network in PyTorch, we create a class that inherits from nn.Module. We define the layers of the network in the __init__ function and specify how data will pass through the network in the forward function. To accelerate operations in the neural network, we move it to the accelerator such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU."
      ],
      "metadata": {
        "id": "kgxdLX_iug_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3BiWeMwuPdQ",
        "outputId": "d5da6242-2dcf-4512-f092-8450c1d4c165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPTIMIZING THE MODEL PARAMETERS"
      ],
      "metadata": {
        "id": "XSg92iQnuoXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we need the loss function and an optimizer"
      ],
      "metadata": {
        "id": "05VTXrJvuw1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "OdTNlRosutsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the modelâ€™s parameters."
      ],
      "metadata": {
        "id": "ZdqEyX36u6Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ],
      "metadata": {
        "id": "5vKqWcF3vNuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also check the model performance to againts the test data set to ensure it is learning"
      ],
      "metadata": {
        "id": "QXWQ4vLkvaAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "y6xuT3FJvkZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training process is conducted over several iterations (epocs). During each epoch, the model learns parameters to make better predictions.\n",
        "We print the model's accuracy and loss at each epoch; we'd like to see the accuracy increase and the loss decrease with every epoch"
      ],
      "metadata": {
        "id": "OWFe32W3wUYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28pmof71xAQb",
        "outputId": "9e4f9273-a388-4955-c75b-5c51d4a6ddba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.304215  [   64/60000]\n",
            "loss: 2.286921  [ 6464/60000]\n",
            "loss: 2.270357  [12864/60000]\n",
            "loss: 2.264478  [19264/60000]\n",
            "loss: 2.255190  [25664/60000]\n",
            "loss: 2.218868  [32064/60000]\n",
            "loss: 2.229124  [38464/60000]\n",
            "loss: 2.199551  [44864/60000]\n",
            "loss: 2.188110  [51264/60000]\n",
            "loss: 2.150312  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.9%, Avg loss: 2.153815 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.169574  [   64/60000]\n",
            "loss: 2.155012  [ 6464/60000]\n",
            "loss: 2.096478  [12864/60000]\n",
            "loss: 2.107938  [19264/60000]\n",
            "loss: 2.080554  [25664/60000]\n",
            "loss: 2.010875  [32064/60000]\n",
            "loss: 2.023860  [38464/60000]\n",
            "loss: 1.954648  [44864/60000]\n",
            "loss: 1.943838  [51264/60000]\n",
            "loss: 1.867742  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.879318 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.913453  [   64/60000]\n",
            "loss: 1.884010  [ 6464/60000]\n",
            "loss: 1.762491  [12864/60000]\n",
            "loss: 1.798203  [19264/60000]\n",
            "loss: 1.724592  [25664/60000]\n",
            "loss: 1.652154  [32064/60000]\n",
            "loss: 1.652908  [38464/60000]\n",
            "loss: 1.564639  [44864/60000]\n",
            "loss: 1.577505  [51264/60000]\n",
            "loss: 1.472661  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.6%, Avg loss: 1.503722 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.566444  [   64/60000]\n",
            "loss: 1.538082  [ 6464/60000]\n",
            "loss: 1.379261  [12864/60000]\n",
            "loss: 1.461536  [19264/60000]\n",
            "loss: 1.367862  [25664/60000]\n",
            "loss: 1.333927  [32064/60000]\n",
            "loss: 1.341774  [38464/60000]\n",
            "loss: 1.270242  [44864/60000]\n",
            "loss: 1.301189  [51264/60000]\n",
            "loss: 1.210014  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.3%, Avg loss: 1.240135 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.310542  [   64/60000]\n",
            "loss: 1.299434  [ 6464/60000]\n",
            "loss: 1.122345  [12864/60000]\n",
            "loss: 1.246004  [19264/60000]\n",
            "loss: 1.137417  [25664/60000]\n",
            "loss: 1.134085  [32064/60000]\n",
            "loss: 1.153966  [38464/60000]\n",
            "loss: 1.091507  [44864/60000]\n",
            "loss: 1.127906  [51264/60000]\n",
            "loss: 1.059628  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 1.078317 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVING MODELS"
      ],
      "metadata": {
        "id": "iaLmlEwUxQCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aGVZHgYxTMX",
        "outputId": "501266d0-ecaf-41d9-9a9b-05f8d18ef966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING MODELS"
      ],
      "metadata": {
        "id": "d4O1gnVMxYvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48BrwILrxbJB",
        "outputId": "4dd69734-6d4b-494b-c64a-63727f90e69d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model can now be used to make predictions"
      ],
      "metadata": {
        "id": "Jijf-bXixdpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    x = x.to(device)\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leb8LaPqxtvK",
        "outputId": "ab923e8e-9767-4d5a-91e9-62bf5d9bff74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TENSORS"
      ],
      "metadata": {
        "id": "c3EDXANsx3LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the modelâ€™s parameters."
      ],
      "metadata": {
        "id": "pcnzWUXtWpDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors are similar to NumPyâ€™s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data.\n",
        "ensors are also optimized for automatic differentiation."
      ],
      "metadata": {
        "id": "8P-8PhwmWp4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Zf1fOZtsx4pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializinga a Tensor\n",
        "\n",
        "Tensors can be initialized in various ways\n"
      ],
      "metadata": {
        "id": "5656GZyFWy7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Directly from data**"
      ],
      "metadata": {
        "id": "v3ZzXOoyW7dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[1, 2],[3, 4]]\n",
        "x_data = torch.tensor(data)"
      ],
      "metadata": {
        "id": "Hx_bdyPSW9bS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From a NumPy array**"
      ],
      "metadata": {
        "id": "_4FcFxx2W_gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ],
      "metadata": {
        "id": "g4F9klrPXFhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From another tensor**"
      ],
      "metadata": {
        "id": "WreXVo-VXHbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ],
      "metadata": {
        "id": "hVCNJqe7XKra",
        "outputId": "3634774c-86c1-434b-c6b5-f4bf9d79d8cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.3493, 0.4903],\n",
            "        [0.1048, 0.4242]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With random or constant values**"
      ],
      "metadata": {
        "id": "qas1iu_YXRIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ],
      "metadata": {
        "id": "nZJup6NaXOpX",
        "outputId": "28917385-c8ec-4235-b891-48afef3fc319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[6.1185e-01, 2.5785e-04, 1.0633e-01],\n",
            "        [4.3547e-01, 4.8780e-01, 1.4198e-01]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attributes of a Tensor\n",
        "\n",
        "Tensor attributes describe their shape, datatype, and the device on which they are stored"
      ],
      "metadata": {
        "id": "1cYcPAnAXVK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ],
      "metadata": {
        "id": "Xp1p8Go0XcHq",
        "outputId": "347cae6c-3084-4463-9b21-f4d13f540d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operations on Tensors\n",
        "Over 1200 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing) sampling and more.\n"
      ],
      "metadata": {
        "id": "58SLCB0vXdM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We move our tensor to the current accelerator if available\n",
        "if torch.accelerator.is_available():\n",
        "    tensor = tensor.to(torch.accelerator.current_accelerator())"
      ],
      "metadata": {
        "id": "3bR-fyu4XnLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standard numpy-like indexing and slicing**"
      ],
      "metadata": {
        "id": "9mNGervaXpOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "print(f\"First row: {tensor[0]}\")\n",
        "print(f\"First column: {tensor[:, 0]}\")\n",
        "print(f\"Last column: {tensor[..., -1]}\")\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "tJES6IneXtEN",
        "outputId": "68695529-c8e9-4d84-ad16-e266556a39b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Joining tensors**\n",
        " You can use `torch.cat` to concatenate a sequence of tensors along a given dimension. See also `torch.stack`, another tensor joining operator that is subtly different from the previous one."
      ],
      "metadata": {
        "id": "1BnR3i9nXxFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "\n",
        "t11 = torch.stack([tensor, tensor, tensor], dim=1)\n",
        "print(t11)"
      ],
      "metadata": {
        "id": "g2J7XYNgX728",
        "outputId": "9b980420-0260-459c-9e29-cc8d12eeeca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "tensor([[[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]],\n",
            "\n",
            "        [[1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.],\n",
            "         [1., 0., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Arithmetic operations**"
      ],
      "metadata": {
        "id": "ZGZgy6IRYHP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
        "# ``tensor.T`` returns the transpose of a tensor\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(y1)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "\n",
        "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n"
      ],
      "metadata": {
        "id": "CaOCkkYlYJhn",
        "outputId": "1ee6f4a5-b0d0-47fa-8d04-bd9c13756c11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.],\n",
              "        [1., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Single-element tensors**\n",
        "\n",
        "If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`"
      ],
      "metadata": {
        "id": "DIb5xR_dYNIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))"
      ],
      "metadata": {
        "id": "37HUcdIPYMQs",
        "outputId": "00d5d9c9-f166-4fd7-da1f-46cd4c66e97b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In-place operations**\n",
        "\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`,`x.t_()` , will change `x`."
      ],
      "metadata": {
        "id": "Ol91IXh3YWKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor} \\n\")\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ],
      "metadata": {
        "id": "jw0L_cMCY3wX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980f4566-898b-42e1-9a54-7b3ecf99f926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE\n",
        "\n",
        "***In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged.***"
      ],
      "metadata": {
        "id": "fGbLG2zDY5xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bridge with NumPy\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the othe"
      ],
      "metadata": {
        "id": "tRQPgXLBY9JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensor to Numpy array**"
      ],
      "metadata": {
        "id": "sI1ndmmclyki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.ones(5)\n",
        "print(f\"t: {t}\")\n",
        "n = t.numpy()\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjyWo7XalVh4",
        "outputId": "038845cc-e6e1-47fc-b3df-06b4ce6287a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A change in the tensor reflects in the NumPy array."
      ],
      "metadata": {
        "id": "reDcBApgl-pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t.add_(1)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-36xV6wmBlR",
        "outputId": "b6ea1bd6-80f7-4f95-8941-e86be2a723a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NumPy array to Tensor**"
      ],
      "metadata": {
        "id": "DESRl-r9mCKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = np.ones(5)\n",
        "t = torch.from_numpy(n)"
      ],
      "metadata": {
        "id": "LcqCCfpsmGgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes in the NumPy array reflects in the tensor"
      ],
      "metadata": {
        "id": "bfSeVErymIDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQwM4Mo7mJtI",
        "outputId": "2c48280e-a117-49ed-e89a-060dafad0d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets & DataLoaders\n",
        "\n",
        "Ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the Dataset to enable easy access to the samples.\n"
      ],
      "metadata": {
        "id": "ZC5CpEzdmcxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a Dataset\n",
        "\n",
        "How to load Fashion-MNIST dataset from TorchVision"
      ],
      "metadata": {
        "id": "PEbeGLw_m50E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "FOaOsSrznFn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterating and Visualizing the Dataset\n",
        "\n",
        "We can index `Datasets` manually like a list: `training_data[index]`. We use matplolib to visualize some samples in our training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "reXVzzNSnHjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "EKIOnATQnHQp",
        "outputId": "6fbdc662-6834-45c2-f7f1-766b37a7e55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Custom Dataset for your files\n",
        "\n",
        "A custom Dataset class must implement three functions: `__init__`, `__len__`, and `__getitem__`. Take a look at this implementation; the FashionMNIST images are stored in a directory `img_dir`, and their labels are stored separately in a CSV file `annotations_file`."
      ],
      "metadata": {
        "id": "oAKSboUGng07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "whXW4YKKpNrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`__init__`\n",
        "\n",
        "The __int_function is run once when instatiating the Dataset object.\n",
        "We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section)."
      ],
      "metadata": {
        "id": "-GPwwcP5pbxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "    self.img_labels = pd.read_csv(annotations_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform"
      ],
      "metadata": {
        "id": "-_qU-DUlqncR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`__len__`\n",
        "\n",
        "The `__len__` function returns the number of samples in our dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "20UnF6MgrWQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __len__(self):\n",
        "    return len(self.img_labels)"
      ],
      "metadata": {
        "id": "RNXq6rlcrntz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##`__getitem__`\n",
        "\n",
        "The`__getitem__` function loads and returns a sample from the dataset at the given index `idx`.\n",
        "Basde on the index it identifies the image's location on disk converts that to a tensor using `read_image retrieves the corresponding label from the csv data in `self.image_labels, calls the transform function on them (if applicable), and returns the tensor image and corresponding label in a tuple\n"
      ],
      "metadata": {
        "id": "ISern-KYrraS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "    label = self.img_labels.iloc[idx, 1]\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    if self.target_transform:\n",
        "        label = self.target_transform(label)\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "MBGnK6LotAv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensors\n",
        "\n",
        "Transforms are used to perform some manipulation of the data and make it suiable for training.\n",
        "\n",
        "The parameters:\n",
        "\n",
        "-`transform` modify the features\n",
        "_`target_transform` modify the labels"
      ],
      "metadata": {
        "id": "0fSl2bu9DBhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FashionMNIST features are in PIL Image format, and the labels are integers"
      ],
      "metadata": {
        "id": "5CE0M-rlEBRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n",
        ")"
      ],
      "metadata": {
        "id": "H4uhbz2IDBO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ToTensor()\n",
        "\n",
        "ToTensor converts PIL image or NumPy `ndarray` into a `FloatTensor`and scales the imagesÂ´s pixels intensity values in the range[0., 1.]\n",
        "\n",
        "##Lambda Transforms\n",
        "\n",
        "apply any user-defined lamba functions\n",
        "Below we define a function to turn the integer into a one-hot encoded tensor.\n",
        "First it creates a zero tensor of size 10 (number of labels in the dataet ) and calls `scatter_` which assigns a `value=1` on the index as given by the label `y`"
      ],
      "metadata": {
        "id": "8LVl5Fn7Dtil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_transform = Lambda(lambda y: torch.zeros(\n",
        "    10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1))"
      ],
      "metadata": {
        "id": "n-6Un7ozEuYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build a Neural Network\n",
        "\n",
        "Neural Networks compromise of layers/modelues that perform operations on data.\n",
        "The `torch.nn` namespace provides all the building blocks you need to build your own neural network\n",
        "Every module in PyTorch subclass the nn.Module.\n",
        "A NN is a model itself that consists of other modules (layers). This nested structure allows for building and managinf complex architectures easily.\n",
        "\n",
        "\n",
        "Build a NN to classify the images on the FashionMNIST dataset."
      ],
      "metadata": {
        "id": "JMoQ45UTE5yG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "SValdgDbFoqw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Get Device dot Training\n",
        "\n",
        "We waant to train the model on a accelarator (CUDA, MPS, MTIA, or XPU)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZU29P21WFrUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "oP9YQILSF4n_",
        "outputId": "6528ee07-c95b-4607-de32-7d6759db894c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the class\n",
        "\n",
        "We define our neural network by subclassing `nn.Module` and initialize the NN layers in `__init__`\n",
        "Every `nn.Module` subclass implements the operations on the input data in the forward method.\n"
      ],
      "metadata": {
        "id": "fi2nAyNyF6Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "FzS5UZfYGgq8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "_LspJpcQH8IV",
        "outputId": "39198a3d-3c01-4fb5-85c3-79659f57ecce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the model we pass it the input data\n",
        "This executes the model's forward, along with some background operations.\n",
        "**DO NOT CALL `model.forward()`DIRECTLY**\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0 correpsonding to each output raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get prediction probabilities by passing it through an instance of the `nn.Softmax`\n",
        "\n"
      ],
      "metadata": {
        "id": "omLoEU5vNK8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "id": "hJIAZ1-gOVn-",
        "outputId": "a6d57df1-cb27-4525-88dd-02e0da469cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model layers\n",
        "\n",
        "We''l take a minibatch of 3 images of size 28x28 and see what happens to it as we pass it through the network\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xj9LbzrmO3Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "id": "OEGhbHWgPYuj",
        "outputId": "eb140f9c-9d94-425b-fd33-95cb102142e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nn.Flatten\n",
        "\n",
        "We initialize the `nn.Faltten` layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (the minibatch dimension (at dim=0) is maintained)\n",
        "\n"
      ],
      "metadata": {
        "id": "rTqTEXkhPmlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "id": "qkZ5j--oP8dU",
        "outputId": "f7fe3c18-0ad0-4871-8d61-43ad2736f517",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nn.Linear\n",
        "\n",
        "The linear layer is a modulethat applies a linear transformation on the input using it stored weights and biases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p2-NRbznQnWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "id": "bwHT1ATcZhBq",
        "outputId": "8e136222-f14b-4d06-ca55-706a6b36036d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
      ],
      "metadata": {
        "id": "wncbxxP0bc1L"
      }
    }
  ]
}